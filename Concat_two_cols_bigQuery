from __future__ import absolute_import
from __future__ import print_function
import apache_beam as beam
from apache_beam.io import ReadFromText
from apache_beam.io import WriteToText
from apache_beam.pipeline import PipelineOptions
from apache_beam.io.gcp.internal.clients import bigquery
with beam.Pipeline(options=PipelineOptions()) as p:
	class Split(beam.DoFn):
		def process(self, element):
			Outlet, CatLib, ProdKey, Week, SalesComponent, DuetoValue, PrimaryCausalKey, CausalValue, ModelIteration, Published = element.split(",")
			return [{ 'Outlet': Outlet,
			'CatLib': CatLib,
			'ProdKey': ProdKey,
			'Week' : Week,
			'SalesComponent': SalesComponent,
			'DuetoValue' : DuetoValue,
			'PrimaryCausalKey' : PrimaryCausalKey,
			'CausalValue' : CausalValue,
			'ModelIteration' : ModelIteration,
			'Published' : Published
			}]
	ip1 = p | 'readFile1' >> beam.io.ReadFromText('gs://tegclorox/Final input/WeeklyDueto.csv')
	ip2 = ip1 |'Split' >> beam.ParDo(Split())
	out = ip2 |'output' >> beam.io.WriteToBigQuery(
				table='table4',
				project='model-shelter-184507',
				dataset='Raw_weekly_dueto',
				schema= 'Outlet:STRING, CatLib:STRING, ProdKey:STRING, Week:STRING, SalesComponent:STRING, DuetoValue:STRING, PrimaryCausalKey:STRING, CausalValue:STRING,	ModelIteration:STRING, Published:STRING',
 				create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
				write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND)
	bip = p | 'aaa' >> beam.io.Read(beam.io.BigQuerySource(query='SELECT *, concat(CatLib,".",ProdKey) as CatLibKey FROM model-shelter-184507.Raw_weekly_dueto.table4'))
	out_bip = bip | 'dswdd' >> beam.io.WriteToText('gs://tegclorox/Output/abcd')
